{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation\n",
    "Appel de l'ensemble des librairies necessaires. Configuration de l'accelereation materielle GPU avec CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\python310\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: torchinfo in c:\\python310\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (1.26.1)\n",
      "Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: torch in c:\\python310\\lib\\site-packages (2.1.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\python310\\lib\\site-packages (0.16.1+cu118)\n",
      "Requirement already satisfied: tqdm in c:\\users\\victor\\appdata\\roaming\\python\\python310\\site-packages (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python310\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python310\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python310\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python310\\lib\\site-packages (from matplotlib) (4.44.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\victor\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\python310\\lib\\site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python310\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\victor\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\python310\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: typing-extensions in c:\\python310\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: fsspec in c:\\python310\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: networkx in c:\\python310\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\victor\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: requests in c:\\users\\victor\\appdata\\roaming\\python\\python310\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\victor\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\victor\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests->torchvision) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\victor\\appdata\\roaming\\python\\python310\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests->torchvision) (1.26.18)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib torchinfo numpy pandas torch torchvision tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des déependances requises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.utils.data as data\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des versions de python et de pytorch ainsi que la disponibilité de l'accélération matérielle GPU avec CUDA.\n",
    "\n",
    "Ceci est utile uniquement pour vérifier que l'environnement est correctement configuré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n",
      "Pytorch 2.1.1+cu118\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Python:\", sys.version)\n",
    "print(\"Pytorch\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "default_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Parametres de l'espace\n",
    "## Modes d'execution\n",
    "\n",
    "- **Mode local**, Commenter la ligne 1 et decommenter la ligne 2\n",
    "- **Mode Kaggle**, Commenter la ligne 2 et decommenter la ligne 1\n",
    "\n",
    "## Training Set\n",
    "\n",
    "Permet d'utiliser ou non l'ensemble des donnees d'entrainement (utile lorsque qu'il faut effectuer des tests d'implementation rapide)\n",
    "\n",
    "## BatchSize\n",
    "\n",
    "Valeur de la batchsize (nombre de donnees dans un lot par iteration)\n",
    "Une petite valeur de cette batch_size signifie que l'on aura de petits lots. Cela consomme moins de memoire la convergence du modele peut etre longue. Il nous faut donc une batch_size d'une certaine taille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = \"/kaggle/input/\"\n",
    "base_path = \"../\"\n",
    "base_path_data = base_path + \"data/\"\n",
    "\n",
    "full_set = True\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "model_name = \"axelnet_40_epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1aa704be7f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions\n",
    "\n",
    "Définition de fonctions utilitaires pour l'affichage des graphiques de résultats, lancer un entrainement et valider le modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## plot_results\n",
    "\n",
    "Permet d'afficher les résultats d'un entrainement et de validation.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- **hist** : l'histogramme des résultats d'entrainement comprenant `train_loss`, `train_acc`, `val_loss` et `val_acc` pour chaque epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(hist):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(hist[\"train_acc\"], label=\"Training acc\")\n",
    "    plt.plot(hist[\"val_acc\"], label=\"Validation acc\")\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(hist[\"train_loss\"], label=\"Training loss\")\n",
    "    plt.plot(hist[\"val_loss\"], label=\"Validation loss\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_epoch\n",
    "\n",
    "Permet de lancer un epoch d'entrainement.\n",
    "\n",
    "Le réseau est mis en mode entrainement et les gradients sont calculés.\n",
    "\n",
    "### arguments\n",
    "\n",
    "- **net**: le modèle à entrainer\n",
    "- **dataloader**: le data loader à utiliser pour l'entrainement qui contient les images d'entrainement\n",
    "- **lr**: le learning rate à utiliser, par défaut 0.01\n",
    "- **optimizer**: l'optimiseur à utiliser, par défaut Adam\n",
    "- **loss_fn**: la fonction de loss à utiliser, par défaut NLLLoss\n",
    "\n",
    "### returns\n",
    "\n",
    "- **loss**: la valeur de loss pour cet epoch\n",
    "- **accuracy**: la valeur d'accuracy pour cet epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    net,\n",
    "    dataloader,\n",
    "    lr=0.01,\n",
    "    optimizer=None,\n",
    "    loss_fn=nn.NLLLoss(),\n",
    "):\n",
    "    print(\"Training:\")\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    net.train()\n",
    "    total_loss, acc, count = 0, 0, 0\n",
    "    for i, (features, labels) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        lbls = labels.to(default_device, non_blocking=True)\n",
    "        out = net(features.to(default_device))\n",
    "        loss = loss_fn(out, lbls)  # cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        acc += (predicted == lbls).sum()\n",
    "        count += len(labels)\n",
    "    return total_loss.item() / count, acc.item() / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validate\n",
    "\n",
    "Permet d'obtenir les résultats du modèle avec les donnnées de validation.\n",
    "\n",
    "Les gradients ne sont pas calculés.\n",
    "\n",
    "### arguments\n",
    "\n",
    "- **net**: le modèle entrainé.\n",
    "- **dataloader**: le data loader à utiliser pour l'entrainement qui contient les images de validation\n",
    "- **loss_fn**: la fonction de loss à utiliser, par défaut NLLLoss. Doit être la même que celle utilisée pour l'entrainement pour avoir des résultats pertinents.\n",
    "\n",
    "### returns\n",
    "\n",
    "- **loss**: la valeur de loss pour cet epoch\n",
    "- **accuracy**: la valeur d'accuracy pour cet epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, dataloader, loss_fn=nn.NLLLoss()):\n",
    "    print(\"Validation:\")\n",
    "    net.eval()\n",
    "    count, acc, loss = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i, (features, labels) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            lbls = labels.to(default_device)\n",
    "            out = net(features.to(default_device))\n",
    "            loss += loss_fn(out, lbls)\n",
    "            pred = torch.max(out, 1)[1]\n",
    "            acc += (pred == lbls).sum()\n",
    "            count += len(labels)\n",
    "    return loss.item() / count, acc.item() / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train \n",
    "\n",
    "permet de lancer l'entrainement du modèle sur un nombre déterminer d'epochs.\n",
    "\n",
    "### arguments\n",
    "\n",
    "- **net**: le modèle à entrainer\n",
    "- **train_loader**: le data loader à utiliser pour l'entrainement qui contient les images d'entrainement\n",
    "- **val_loader**: le data loader à utiliser pour l'entrainement qui contient les images de validation\n",
    "- **optimizer**: l'optimiseur à utiliser, par défaut Adam\n",
    "- **lr**: le learning rate à utiliser, par défaut 0.01\n",
    "- **epochs**: le nombre d'epochs à effectuer, par défaut 10\n",
    "- **loss_fn**: la fonction de loss à utiliser, par défaut NLLLoss.\n",
    "\n",
    "### returns\n",
    "\n",
    "- **train_loss**: la liste des valeurs de loss pour chaque epoch\n",
    "- **train_accuracy**: la liste des valeurs d'accuracy pour chaque epoch\n",
    "- **val_loss**: la liste des valeurs de loss pour chaque epoch\n",
    "- **val_accuracy**: la liste des valeurs d'accuracy pour chaque epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    net,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer=None,\n",
    "    lr=0.01,\n",
    "    epochs=10,\n",
    "    loss_fn=nn.NLLLoss(),\n",
    "):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    res = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    for ep in range(epochs):\n",
    "        tl, ta = train_epoch(\n",
    "            net, train_loader, optimizer=optimizer, lr=lr, loss_fn=loss_fn\n",
    "        )\n",
    "        vl, va = validate(net, val_loader, loss_fn=loss_fn)\n",
    "        print(\n",
    "            f\"Epoch {ep:2}, Train acc={ta:.3f}, Val acc={va:.3f}, Train loss={tl:.3f}, Val loss={vl:.3f}\"\n",
    "        )\n",
    "        res[\"train_loss\"].append(tl)\n",
    "        res[\"train_acc\"].append(ta)\n",
    "        res[\"val_loss\"].append(vl)\n",
    "        res[\"val_acc\"].append(va)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mise en place des transformations à appliquer aux images d'entrée.\n",
    "\n",
    "1. Transformation en tenseur\n",
    "2. Redimensionnement des images en 256x256\n",
    "3. Coupe centrale de 256x256 si l'image est plus grande ou non carré\n",
    "4. Normalisation des valeurs des pixels entre 0 et 1. Grâce à une moyenne de 0.5 et un écart type de 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose(\n",
    "    [\n",
    "        T.ToTensor(),\n",
    "        T.Resize(256, antialias=True),\n",
    "        T.CenterCrop(256),\n",
    "        T.Normalize(0.5, 0.5),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des données d'entrainement fournies via `ImageFolder` en appliquant les transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_all = ImageFolder(root=base_path_data + \"/train\", transform=transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre d'images par classes n'est pas équilibré, il y a donc un poids à appliquer à chaque classe pour que le modèle ne soit pas biaisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class_counts = Counter(train_dataset_all.targets)\n",
    "\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Class Repartition\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création des poids de répartition des classes\n",
    "\n",
    "Comme la répartition des classes n'est pas équilibrée, il faut créer des poids pour chaque classe afin de compenser ce déséquilibre pour la loss_function.\n",
    "\n",
    "Une autre solution aurait été de créer des images supplémentaires pour les classes sous représentées. Ce procédé de DataAugmentation repose souvent sur de la translation spatiale des images (les retourner, inverser la symmetrie, etc...). Pour certains cas, on peut également jouer sur les canaux de colorimétrie (négatif par exemple).\n",
    "\n",
    "Par soucis de simplicité, nous avons choisi de créer des poids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_class_count = max(class_counts.values())\n",
    "total_samples = sum(class_counts.values())\n",
    "class_weights = [class_counts[label] / max_class_count for label in class_counts.keys()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Découpe des données d'entrainement en données d'entrainement et données de validation avec un ratio de 80/20.\n",
    "\n",
    "La découpe est faites aléatoirement avec un seed fixé pour avoir toujours les mêmes résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split the dataset in train and val set\n",
    "train_size = int(0.8 * len(train_dataset_all))\n",
    "val_size = len(train_dataset_all) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset_all, [train_size, val_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création des data loaders pour l'entrainement et la validation sur base du paramètre `full_set` qui permet de choisir si l'on utilise l'ensemble des données d'entrainement ou seulement une partie.\n",
    "\n",
    "Ceci n'est utile que pour effectuer des tests d'implémentation rapide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_set == True:\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True\n",
    "    )\n",
    "else:\n",
    "    # get the indices of 1000 random samples\n",
    "    indices_train = torch.randperm(len(train_dataset))[:1024]\n",
    "    indices_val = torch.randperm(len(val_dataset))[:200]\n",
    "    # create a sampler using the indices\n",
    "    sampler_train = data.SubsetRandomSampler(indices_train)\n",
    "    sampler_val = data.SubsetRandomSampler(indices_val)\n",
    "\n",
    "    # create a dataloader using the sampler\n",
    "    train_loader = data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=sampler_train\n",
    "    )\n",
    "    val_loader = data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, sampler=sampler_val\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du modèle\n",
    "## Choix du modèle\n",
    "Afin de traiter le probleme intial, nous avons fait le choix de partir sur une architecture similaire a AlexNet. Vaincqueur du ILSVRC en 2012, ce classifieur est composé de 8 couches dont 5 de convolution. L'intérêt d'un modèle comme celui-ci est qu'il propose de bons résultats ton en étant rainsonnable sur sa complexité : \n",
    "-  La taille des filtres n'est pas trop importante ce qui permet d'effectuer des convolutions rapides\n",
    "-  La taille des filtres permet également de projeter des caractéristiques plus discriminantes (par rapport à de plus grands filtres)\n",
    "-  Le nombre de couches est raisonnable ce qui évite l'overfitting et accélère l'entrainement\n",
    "-  Il peut être couplé avec une fonction de DropOut permettant de \"Désactiver\" certains neuronne pour éviter l'overfitting\n",
    "\n",
    "Ainsi, malgré que ce modèle soit assez ancien (de manière relative), ses performances restent très pertinentes avec une complexité contenue\n",
    "\n",
    "![AlexNetModel](https://miro.medium.com/v2/resize:fit:960/0*d0G7SqdXlK67J5aN.png)\n",
    "Modèle AxelNet (https://miro.medium.com/v2/resize:fit:960/0*d0G7SqdXlK67J5aN.png)\n",
    "\n",
    "## Couche de convolution et pooling\n",
    "Les différentes couches de convolution permettent de former des filtre qui vont saisir des motifs caractéristiques de notre image. On applique des opérations de pooling à sortie des couches de convolution afin de réduire la dimension de l'image et conserver les caractéristiques importantes. L'opération de convolution est une opération de base en traitement du signal (utilisée en filtrage par exemple). Cette opération peut rapidement devenir assez couteuse et il est donc important de bien choisir la taille des couches de convolution : \n",
    "- On commence avec une couche de convolution prenant 64 canaux d'entrée et surtout un noyau de convolution important. Plus il est important, moins on va saisir de détails (la convolution s'effectue avec un noyau plus grand donc on forme notre image avec les informations de 25 pixels avec un noyeau de 5)\n",
    "- Après la couche de pooling, notre image est un vecteur caractérisitque projeté dans un espace plus petit contenant moins de détails. Il nous faut dès lors saisir des détails de manière plus précise, on vient donc diminuer la taille du noyeau de convolution. Ainsi, on prendra l'information de 9 pixels pour la sortie de notre image, on capture donc plus de détails.\n",
    "\n",
    "\n",
    "Avec une 2e couche, on analyse et détecte des formes plus grandes et ainsi de suite.\n",
    "\n",
    "De plus, comme on combine les formes basiques obtenues par la première couche, on analyse des formes de plus en plus complexes et grandes.Donc, plus on avance dans les couches de convolution, plus on va vers une sorte de sémantique\n",
    "\n",
    "## Couche ReLu\n",
    "\n",
    "La couche ReLu permettant d'introduire de la non-linératié est assez intéressante dans notre contexte. Tout d'abord, elle peu couteuse en calcul de part sa simplicité (f(x)=max(0,x)) par rapport à une Sigmoid par exemple.\\\n",
    "\n",
    "En effet, les fonctions d'activation telles que la sigmoïde peuvent entraîner un problème du gradient qui diminue avec la profondeur du réseau ce qui pose porblème. La fonction ReLU évite ce problème en permettant au gradient de passer à travers pour les valeurs positives.\n",
    "\n",
    "## Average pooling et LinearConnection\n",
    "À la sortie de nos couches de convolution et du MaxPooling, nous obtenons une représentations réduite de notre image. Celle-ci est constituée des caractérisitques extraite par les opérations de convolution successive et la réduction de notre espace.\n",
    "La couche d'average pooling va prendre la moyenne des activations pour chaque canaux et faire retomber la dimension de notre sortie à 256x6x6. On obtient un vecteur caractéristique petit mais très dense en caractéstiques.\n",
    "\n",
    "Enfin, les couches Linear permettent de passer de notre vecteur 256x6x6 au nombre de classes souhaitées. Il s'agit de simple couches fully connected.\n",
    "\n",
    "## Reduction spatiale et canaux\n",
    "Un point intéressant à noter dans le modèle présenté, est la diminution progressive de la dimension spatiale de l'image (sortie d'image en 6x6) et l'augmentation du nombre de canaux (384 au maximum puis se terminant à 256). \n",
    "Cette méthode permet de créer de la profondeur (matérialisée par les canaux) et ainsi saisir des motifs complexes robustes à la translation. \n",
    "\n",
    "Ainsi, en plus d'accélerer les calculs en réduisant les dimensions des images sur lesquelles nous appliquons des opérations, nous permettons au modèle de mieux se spécialiser (canaux). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.conv1(x)))\n",
    "        x = self.maxpool(self.relu(self.conv2(x)))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.maxpool(self.relu(self.conv5(x)))\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the network\n",
    "net = AlexNet(num_classes=39)\n",
    "net = net.to(default_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary permet d'afficher un résumé du modèle avec le nombre de paramètres et la taille de la sortie de chaque couche.\n",
    "\n",
    "Cela permet d'avoir une estimation de la puissance de calcul ainsi que de la mémoire nécessaire pour l'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "AlexNet                                  [256, 39]                 --\n",
       "├─Conv2d: 1-1                            [256, 64, 63, 63]         23,296\n",
       "├─ReLU: 1-2                              [256, 64, 63, 63]         --\n",
       "├─MaxPool2d: 1-3                         [256, 64, 31, 31]         --\n",
       "├─Conv2d: 1-4                            [256, 192, 31, 31]        307,392\n",
       "├─ReLU: 1-5                              [256, 192, 31, 31]        --\n",
       "├─MaxPool2d: 1-6                         [256, 192, 15, 15]        --\n",
       "├─Conv2d: 1-7                            [256, 384, 15, 15]        663,936\n",
       "├─ReLU: 1-8                              [256, 384, 15, 15]        --\n",
       "├─Conv2d: 1-9                            [256, 256, 15, 15]        884,992\n",
       "├─ReLU: 1-10                             [256, 256, 15, 15]        --\n",
       "├─Conv2d: 1-11                           [256, 256, 15, 15]        590,080\n",
       "├─ReLU: 1-12                             [256, 256, 15, 15]        --\n",
       "├─MaxPool2d: 1-13                        [256, 256, 7, 7]          --\n",
       "├─AdaptiveAvgPool2d: 1-14                [256, 256, 6, 6]          --\n",
       "├─Linear: 1-15                           [256, 2048]               18,876,416\n",
       "├─ReLU: 1-16                             [256, 2048]               --\n",
       "├─Linear: 1-17                           [256, 1024]               2,098,176\n",
       "├─ReLU: 1-18                             [256, 1024]               --\n",
       "├─Linear: 1-19                           [256, 39]                 39,975\n",
       "==========================================================================================\n",
       "Total params: 23,484,263\n",
       "Trainable params: 23,484,263\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 227.88\n",
       "==========================================================================================\n",
       "Input size (MB): 201.33\n",
       "Forward/backward pass size (MB): 1317.35\n",
       "Params size (MB): 93.94\n",
       "Estimated Total Size (MB): 1612.62\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net, input_size=(batch_size, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lancement de l'entainement\n",
    "\n",
    "Lancement de l'entrainement du modèle avec les paramètres suivants:\n",
    "\n",
    "- **epochs**: Nombre d'Epochs\n",
    "Dans notre cas (petit nombre de données), nous avons choisit de ne pas dépasser 20 sur le nombre d'itérations. En effet, au delà, le modèle over-fit et notre loss_function ne diminue plus.\n",
    "Ainsi, ce paramètre est à ajuster en fonction de notre jeu de données et de la profondeur de nombre modèle.\n",
    "\n",
    "- **lr**: Learning Rate\n",
    "Nous avons ici choisi de mettre un LearningRate assez faible afin de bien laisser le temps au modèle de converger. Nous pourrions mettre en place un LearningRate adaptatif pour que celui-ci augmente en fonction des résulats (augmente lorsque l'erreur est importante) afin d'accelerer la convergence.\n",
    "À contrario, dans le cas où nous aurions choisit un LR trop petit, nous aurions pu nous bloquer dans un optimum local...\n",
    "\n",
    "- **loss_fn**: Loss Function\n",
    "Enfin, la loss function utilisée est ici une fonction minimisant l'entropie entre la probabilité prédite et la bonne valeur (étiquette). Ainsi, plus la différence de valeur est importante entre notre prédiction et l'étiquette plus l'entropie est grande (incertitude). Cette fonction est très utilisée dans le cadre des classifieur d'images. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:40<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0, Train acc=0.108, Val acc=0.229, Train loss=0.012, Val loss=0.010\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:38<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:13<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Train acc=0.357, Val acc=0.452, Train loss=0.008, Val loss=0.006\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:44<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:10<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2, Train acc=0.523, Val acc=0.575, Train loss=0.006, Val loss=0.005\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:46<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:10<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3, Train acc=0.600, Val acc=0.598, Train loss=0.004, Val loss=0.004\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:37<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:10<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4, Train acc=0.648, Val acc=0.653, Train loss=0.004, Val loss=0.004\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:38<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:10<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5, Train acc=0.693, Val acc=0.691, Train loss=0.003, Val loss=0.003\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 43/71 [00:21<00:14,  1.91it/s]"
     ]
    }
   ],
   "source": [
    "hist = train(\n",
    "    net, train_loader, val_loader, epochs=20, lr=0.0001, loss_fn=nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(default_device))\n",
    ")\n",
    "plot_results(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde des poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "filename = base_path + f\"/deeplearning_{model_name}_{timestamp}.pt\"\n",
    "torch.save(net.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtention des résultat de test\n",
    "Utilisation du réseau sur les données de test pour obtenir les résultats.\n",
    "\n",
    "Les résultats sont ensuite écrit dans un fichier csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = base_path_data + \"/train\"\n",
    "\n",
    "\n",
    "label_names = train_dataset_all.find_classes(train_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = base_path_data + \"/test\"\n",
    "test_path_files = test_path + \"/unknown\"\n",
    "\n",
    "# create a list of all the image paths in the test folder\n",
    "test_images = [os.path.join(test_path_files, f) for f in os.listdir(test_path_files)]\n",
    "\n",
    "# create a list of all the image ids\n",
    "test_ids = [os.path.splitext(os.path.basename(f))[0] for f in test_images]\n",
    "\n",
    "# create a dataset from the test images\n",
    "test_dataset = ImageFolder(root=test_path, transform=transforms)\n",
    "\n",
    "# create a dataloader from the test dataset\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "# create a list to store the predictions\n",
    "predictions = []\n",
    "\n",
    "# loop through the test loader and make predictions\n",
    "with torch.no_grad():\n",
    "    for i, (features, labels) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "        out = net(features.to(default_device))\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        predictions += predicted.cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = [label_names[prediction] for prediction in predictions]\n",
    "\n",
    "\n",
    "# create a list of tuples with the test ids and their predicted labels\n",
    "results = list(zip(test_ids, predicted_labels))\n",
    "\n",
    "# save the results to a CSV file\n",
    "with open(\n",
    "    base_path + f\"results_{model_name}_{timestamp}.csv\", mode=\"w\", newline=\"\"\n",
    ") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Id\", \"Disease (target)\"])\n",
    "    writer.writerows(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexes\n",
    "\n",
    "## Utilisation du model Axelnet pré-entrainé sur ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilisation d'axelnet permet d'obtenir de meilleurs résultats car les poids sont déjà ajustés lors de l'entrainement sur Imagenet. Cela permet de gagner du temps et d'avoir un modèle plus performant en gagnant du temps d'entrainement. \n",
    "\n",
    "Cependant, il faut faire attention à la taille et à la normalisation des images. En effet, les images d'entrainement d'axelnet sont de taille 224x224 et normalisées avec une moyenne de 0.485 et un écart type de 0.229. Il faut donc adapter les images d'entrainement pour qu'elles correspondent à ces paramètres.\n",
    "\n",
    "Le nombre de classes d'AxelNet (1000) diffère du nombre de classes du dataset. Pour y remédier, nous avons 2 solutions:\n",
    "\n",
    "- La plus simple: est de lancé un entrainement comme avec un model non-entrainé et de le laisser modifier ses poids pour s'adapter à nos classes. Vu qu'il y a moins de classes dans notre datasets, les poids de sorties pour les classe après 39 (nombres de labels ici) vont devenir inutiles et ne seront pas utilisés.\n",
    "- La plus précise: est de modifier la dernière couche du modèle pour faire correspondre le nombre de noeuds de sortis avec notre ombre de classes et d'ensuite entrainer le modèle sur notre dataset.\n",
    "\n",
    "ici, nous avons choisi la première solution pour sa simplicité.\n",
    "Malgré cela, les résultast sont plus que satisfaisants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_normalizel_axel = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transforms_axel = T.Compose(\n",
    "    [\n",
    "        T.Resize(256, antialias=True),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        std_normalize_axel,\n",
    "    ]\n",
    ")\n",
    "\n",
    "net = models.alexnet(weights=\"DEFAULT\")\n",
    "net.to(default_device)\n",
    "\n",
    "\n",
    "summary(net, input_size=(batch_size, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = train(\n",
    "    net,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=10,\n",
    "    lr=0.0001,\n",
    "    loss_fn=nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(default_device)),\n",
    ")\n",
    "plot_results(hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
